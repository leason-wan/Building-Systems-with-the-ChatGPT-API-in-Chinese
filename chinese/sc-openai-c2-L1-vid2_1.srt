1
00:00:00,000 --> 00:00:10,080
在这个第一个视频中，我想与您分享一下LLMs（大型语言模型）的概述，我们将介绍它们的训练方式以及诸如分词器以及如何影响LLM提示输出的细节。我们还将看一下LLMs的聊天格式，这是一种指定系统和用户消息并理解其功能的方式。让我们来看一下。首先，大型语言模型是如何工作的？

2
00:00:10,080 --> 00:00:16,240
您可能熟悉文本生成过程，其中您可以给出提示，比如“我喜欢吃”，并要求LLM填写可能的完成方式。

3
00:00:16,240 --> 00:00:21,760
那么，模型是如何学习做到这一点的呢？训练LLM的主要工具实际上是监督学习。

4
00:00:21,760 --> 00:00:28,800
在监督学习中，计算机使用标签训练数据学习输入输出或X或Y映射。因此，如果您使用监督学习来学习分类餐厅评论的情感，您可能会收集像这样的训练集，其中像“最好的中国三明治很棒”这样的评论被标记为积极的情感评论，以此类推。

5
00:00:28,800 --> 00:00:35,040
而“服务很慢，食物一般”则是负面的，“伯爵茶很棒”则是正面的标签。顺便说一下，Izer和我都出生在英国，所以我们都喜欢伯爵茶。
 
16
00:01:35,680 --> 00:01:42,000
因此，监督学习的过程通常是获取标签数据，然后对数据进行训练。

17
00:01:42,000 --> 00:01:48,160
在训练之后，您可以部署和调用模型，并给它一个新的餐厅评论，比如“我吃过的最好的比萨饼”。您希望输出具有积极情感的结果。

18
00:01:48,160 --> 00:01:52,800
事实证明，监督学习是训练大型语言模型的核心构建块。具体而言，可以使用监督学习来重复预测下一个单词来构建大型语言模型。

19
00:01:52,800 --> 00:01:58,880
假设在您的训练集中有大量文本数据，其中有一句话是“我最喜欢的食物是带奶油芝士和莫克斯的百吉饼”。

20
00:01:58,880 --> 00:02:05,040
然后，这个句子被转换成一系列的训练示例，其中给定一个句子片段“我最喜欢的食物是”，如果您想要预测下一个单词，在这种情况下是“百吉饼”，或者给定句子片段或句子前缀“我最喜欢的食物是百吉饼”，下一个单词将是“带”。

21
00:02:05,040 --> 00:02:13,200
在大量的数十亿甚至更多的单词的训练集中，您可以创建一个巨大的训练集，其中您可以从一个句子或一段文本的一部分开始，反复要求语言模型学习预测下一个单词。

22
00:02:13,200 --> 00:02:16,320
因此，今天有两种主要类型的大型语言模型。

23
00:02:17,200 --> 00:02:24,400
第一种是基础LLM，第二种是越来越多地使用的指令。
 
31
00:03:10,720 --> 00:03:16,720
了解。因此，基础LLM根据文本训练数据反复预测下一个单词。

32
00:03:17,920 --> 00:03:22,720
因此，如果我给它一个提示，从前有一只独角兽，那么它可能会反复预测一个单词，从而完成一个关于独角兽的故事，

33
00:03:22,720 --> 00:03:27,120
生活在一个有所有独角兽朋友的神奇森林中。现在这样做的一个缺点是，如果你用什么是法国的首都来提示它，

34
00:03:27,120 --> 00:03:33,040
在互联网上可能会有一个关于法国的测验问题列表。因此，它可能会用什么是法国最大的城市，法国的人口等来完成这个问题。但是你真正想要的是，

35
00:03:33,040 --> 00:03:37,840
你想让它告诉你法国的首都可能而不是列出所有这些问题。

36
00:03:37,840 --> 00:03:42,000
因此，对LLM的指令尝试遵循指令，并希望它会说法国的首都是巴黎。

37
00:03:42,000 --> 00:03:48,080
如何从基础LLM到LLM的指令？这就是像ChatGPT这样的LLM训练指令的过程。首先，

38
00:03:48,080 --> 00:03:53,120
在大量数据上训练基础LLM，可能是数百亿个单词，甚至更多。这是一个可能需要数月时间在大型超级计算系统上进行的过程。在训练了

39
00:03:54,080 --> 00:03:59,280
基础LLM之后，您将进一步通过在一个较小的示例集上微调模型来训练模型，其中输出遵循输入指令。因此，例如，您可能会有承包商

40
00:03:59,280 --> 00:04:04,960
帮助您编写许多指令和良好响应指令的示例。 

41
00:04:05,840 --> 00:04:12,000
这就是训练LLM指令的过程，就像ChatGPT一样。首先在大量数据上训练基础LLM，可能是数百亿个单词，甚至更多。这是一个可能需要数月时间在大型超级计算系统上进行的过程。在训练了基础LLM之后，您将进一步通过在一个较小的示例集上微调模型来训练模型，其中输出遵循输入指令。因此，例如，您可能会有承包商帮助您编写许多指令和良好响应指令的示例。
 
47
00:04:45,440 --> 00:04:50,560
这样就创建了一个训练集，以进行额外的微调，使其学习预测

48
00:04:50,560 --> 00:04:56,800
如果它试图遵循一条指令，下一个单词是什么。之后，为了提高LLM输出的质量，现在通常的过程是获取人类对质量的评价

49
00:04:56,800 --> 00:05:03,200
许多不同的LLM输出的标准，例如输出是否有帮助，诚实和无害。

50
00:05:03,200 --> 00:05:09,520
然后，您可以进一步调整LLM，以增加其生成更高评级输出的概率。而这样做的最常见技术是RLHF，即从人类反馈中进行强化学习。

51
00:05:09,520 --> 00:05:15,920
而训练基础LLM可能需要数月的时间，从基础LLM到指令到LLM的过程可能只需要几天，数据集的规模也更小，计算资源的规模也更小。

56
00:05:39,600 --> 00:05:46,240
这就是如何使用LLM。我将导入一些库。我将在本视频后面再说一些关于这个的事情。这是一个帮助器

57
00:05:46,240 --> 00:05:52,880
给出提示后得到完成的函数。如果您尚未在计算机上安装OpenAI包

58
00:05:52,880 --> 00:06:00,640
，您可能需要运行pip install OpenAI，但我已经安装了它

59
00:06:00,640 --> 00:06:07,760
在这里，所以我不会运行它。让我按shift enter运行这些。现在我可以设置响应

60
00:06:07,760 --> 00:06:14,720
等于得到完成。法国的首都是什么？希望它会给我一个好的
 
62
00:06:32,480 --> 00:06:41,840
结果。现在，关于-现在，在大型语言模型的描述中，我谈到了它作为逐个预测单词，但实际上还有一个更重要的技术细节。

63
00:06:41,840 --> 00:06:46,320
如果你告诉它，取单词棒棒糖中的字母并将它们反转。

64
00:06:46,960 --> 00:06:53,920
这似乎是一个容易的任务。也许像一个四岁的孩子可以完成这个任务。但是如果你让chatGPT去做这个，它实际上会输出一些混乱的东西。这不是L-O-L-I-P-L-P。这不是棒棒糖的字母反转。那么为什么chatGPT无法完成看起来相对简单的任务呢？事实证明，大型语言模型的工作还有一个更重要的细节，它实际上并不是重复预测下一个单词。它反复预测下一个标记。而LLM实际上所做的是，它将像学习新事物很有趣这样的字符序列组合在一起，形成由常见字符序列组成的标记。所以在这里，学习新事物很有趣。它们每个都是一个相当常见的单词。因此，每个标记对应一个单词或一个带空格或感叹号的单词。但是，如果你给它一些使用不太频繁的单词的输入，比如提示是强大的开发人员，那么提示这个词在英语中仍然不太常见，但肯定越来越受欢迎。
 
77
00:08:13,520 --> 00:08:19,680
实际上，它被分解为三个标记，包括 prompt 和 in，因为这三个标记经常出现。

78
00:08:19,680 --> 00:08:27,040
字母序列。如果你给它单词棒棒糖，分词器实际上会将其分解为三个标记，L 和 O 和 Epop。因为 Chai GPT 没有看到单独的字母，而是看到了这三个标记，所以它更难以正确地按相反的顺序打印出这些字母。所以这里有一个技巧可以用来解决这个问题。如果我在这些字母之间添加破折号，空格也可以，或者其他东西也可以，并告诉它取字母和棒棒糖并将它们反转。那么它实际上会做得更好，这个 OOIPOP。原因是如果你传递带有破折号的棒棒糖，它会将每个字符标记化为一个单独的标记，使它更容易看到单独的字母并按相反的顺序打印它们。所以如果你想用 Chai GPT 玩一个单词游戏，比如 Wordle 或 Scrabble，这个巧妙的技巧可以帮助它更好地看到单词的单个字母。对于英语，一个标记大约对应于四个字符或大约三分之三的单词。因此，不同的大型语言模型通常会对它可以接受的输入加上不同的限制，加上输出标记。输入通常称为上下文，输出通常称为完成。 
91
00:09:44,560 --> 00:09:50,080
它可以接受。输入通常称为上下文，输出通常称为完成。
 
92
00:09:50,720 --> 00:09:56,720
例如，最常用的Chai GPT模型GPT 3.5 Turbo的限制是大约4000个输入加输出的标记。因此，如果您尝试提供比这更长的输入上下文，它实际上会抛出异常或生成错误。接下来，我想与您分享使用LLM API的另一种强大方法，它涉及指定单独的系统、用户和助手消息。让我给您展示一个例子，然后我们可以详细解释它实际上在做什么。

93
00:09:56,720 --> 00:10:03,680
这里有一个名为getCompletionFromMessages的新辅助函数。当我们提示这个LLM时，我们将给它多个消息。这里是一个例子，我将首先指定一个系统角色的消息。所以这个助手消息和系统消息的内容，你是一个以Dr. Seuss的风格回应的助手。然后我将指定一个用户消息。所以第二个消息的角色是用户。这个内容是写一个关于快乐胡萝卜的非常短的诗。所以让我们运行它。并且温度等于一，我实际上永远不知道会发生什么。但好吧，那是一首很酷的诗。哦，我看到的这个胡萝卜是多么欢乐啊！它实际上押韵得很好。好的，干得好，chatGPT。因此，在这个例子中，系统消息指定了您希望大型语言模型执行的总体语气。用户消息是
 
107
00:11:33,280 --> 00:11:39,120
在给定这个更高级别的行为的情况下，您想要执行特定的指令，这是由系统消息指定的。

108
00:11:39,120 --> 00:11:46,320
这是它的工作原理的一个例子。这就是聊天格式的工作方式。系统消息设置了大型语言模型或助手的整体行为基调，当您给出用户消息时，例如“告诉我一个笑话”或“写一首诗”，它将根据您要求的内容和系统消息中设置的整体行为输出适当的响应。

109
00:11:46,320 --> 00:11:56,000
顺便说一下，虽然我在这里没有举例说明，但如果您想在多个术语的对话中使用它，您也可以在这些消息格式中输入助手消息，以让chatGPT知道它之前说过什么。如果您还想基于之前说过的内容继续对话。但这里有更多的例子。如果您想将语气设置为让它输出一个句子，那么在系统消息中，我可以说所有您的响应都必须是一个句子长。当我执行此操作时，它会输出一个句子。它不再是一首诗，也不是Dr. Seuss的风格，但它是一个句子。有一个关于快乐胡萝卜的故事。如果我们想同时指定风格和长度，那么我可以使用系统消息来说，在系统响应中，风格为Dr. Seuss，所有句子都必须是一个句子长。
 
122
00:13:22,960 --> 00:13:33,440
现在这生成了一句不错的诗句。它总是微笑着，从不可怕。我喜欢这个。

123
00:13:33,440 --> 00:13:41,520
这是一首非常快乐的诗。最后，只是为了好玩，如果您正在使用 LLM 并想知道您使用了多少令牌，

124
00:13:41,520 --> 00:13:48,480
这里有一个帮助函数，它更加复杂，因为它从 OpenAI API 端点获取响应。然后它使用响应中的其他值来告诉您在 API 调用中使用了多少提示令牌、完成令牌和总令牌。

125
00:13:48,480 --> 00:13:56,400
让我定义一下。如果我现在运行这个，这里是响应。这里是使用了多少令牌的计数。所以这个输出有 55 个令牌，而提示输入有 37 个令牌。所以总共使用了 92 个令牌。当我在实践中使用 LLM 模型时，我并不太担心使用的令牌数量。也许有一种情况值得检查令牌数量，那就是如果您担心用户给出的输入过长，超过了 Chai GPT 的 4,000 个左右的令牌限制，那么您可以再次检查它有多少令牌，并截断它以确保您在大型语言模型的输入令牌限制内。现在，我想与您分享如何使用大型语言模型的另一个提示。

135
00:15:01,520 --> 00:15:09,920
调用 OpenAI API 需要使用与免费或付费帐户相关联的 API 密钥。因此，许多开发人员会以明文形式编写 API 密钥。
 
137
00:15:16,880 --> 00:15:24,960
像这样将其放入他们的Jupyter笔记本中。这是一种使用API密钥的不太安全的方式，我不建议您使用，因为很容易与其他人共享此笔记本或检查

138
00:15:24,960 --> 00:15:32,320
这个到GitHub或其他地方，从而泄漏您的API密钥给其他人。相比之下，

139
00:15:32,320 --> 00:15:39,440
您在Jupyter笔记本中看到我的做法是使用库

140
00:15:39,440 --> 00:15:45,760
.env，然后运行此命令load.env find.env以读取名为.env的本地文件，其中

141
00:15:45,760 --> 00:15:54,800
包含我的秘密密钥。因此，使用此代码片段，我已经本地存储了一个名为

142
00:15:54,800 --> 00:16:02,160
.env的文件，其中包含我的API密钥，并将其加载到操作系统的环境变量中。

143
00:16:02,160 --> 00:16:09,520
然后os.getenv，open AI API key将其存储在此变量中。在整个过程中，

144
00:16:10,720 --> 00:16:19,200
我从未以明文和未加密的明文输入API密钥到我的Jupyter中

145
00:16:19,200 --> 00:16:25,040
笔记本。因此，这是一种相对更安全和更好的访问API密钥的方法。实际上，

146
00:16:25,040 --> 00:16:32,480
这是一种存储许多不同在线服务的不同API密钥并从Jupyter笔记本中调用它们的通用方法。最后，我认为

147
00:16:32,480 --> 00:16:38,640
提示正在革命化AI应用程序开发的程度仍然被低估。在

148
00:16:38,640 --> 00:16:47,120
您可能想要使用并从Jupyter笔记本中调用的许多不同在线服务的不同API密钥的传统监督式机器学习工作流程中，例如餐厅评论情感分类

149
00:16:47,120 --> 00:16:53,680
我刚才提到的例子，如果您想构建分类器来分类餐厅评论
 
152
00:17:04,320 --> 00:17:09,520
正面和负面情感，你首先需要获取一堆标签数据，可能有数百个例子。

153
00:17:09,520 --> 00:17:15,680
这可能需要，我不知道，几周，也许一个月。然后你会在数据上训练一个模型，

154
00:17:15,680 --> 00:17:22,160
获取一个适当的开源模型，调整模型，评估它。这可能需要几天、几周，甚至几个月的时间。

155
00:17:22,160 --> 00:17:29,280
然后你可能需要找到一个云服务来部署它，然后将你的模型上传到云端，然后运行模型，最终能够调用

157
00:17:33,600 --> 00:17:38,720
你的模型。这对于一个团队来说，需要几个月的时间才能完成，这并不罕见。

158
00:17:39,600 --> 00:17:46,080
与基于提示的机器学习相比，当你有一个文本应用程序时，你可以指定一个提示。这可能需要几分钟，也许需要几个小时

160
00:17:51,200 --> 00:17:59,200
来获得有效的提示。然后在几个小时内，最多几天，但实际上更多的是几个小时，

161
00:17:59,760 --> 00:18:06,000
你可以使用API调用来运行它，并开始调用模型。一旦你做到了这一点，只需要几分钟或几个小时，

162
00:18:06,000 --> 00:18:12,080
你就可以开始调用模型并开始进行推理。因此，有些应用程序以前可能需要我花费六个月或一年的时间来构建，

164
00:18:18,240 --> 00:18:23,840
现在你可以在几分钟或几个小时内构建，也许只需要很少的几天时间。这是通过提示正在改变可以快速构建的AI应用程序的范围。一个重要的警告，

166
00:18:30,000 --> 00:18:35,040
这适用于许多非结构化数据应用程序，包括特定的文本应用程序。
 
167
00:18:35,040 --> 00:18:41,280
应用程序，也许越来越多地是视觉应用程序，尽管视觉技术现在还不太成熟，但它正在逐渐发展。这个方案并不适用于结构化数据应用程序，也就是在带有大量数字值的Excel电子表格上进行机器学习应用程序，但对于适用于此的应用程序，人工智能组件可以快速构建，这正在改变整个系统的工作流程。构建整个系统可能仍然需要几天或几周的时间，但至少其中的这一部分可以更快地完成。因此，让我们继续下一个视频，Yisa将展示如何使用这些组件来评估客户服务助手的输入。这将是您通过本课程开发在线零售商客户服务助手的一个更大的示例的一部分。
