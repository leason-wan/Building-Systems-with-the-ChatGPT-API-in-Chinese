1
00:00:00,000 --> 00:00:11,440
在上一个视频中，您了解了如何在一个例子中评估llm输出，其中它有正确的答案，因此可以编写一个函数，明确告诉我们llm输出是否正确分类和列出产品。

2
00:00:11,440 --> 00:00:18,040
但是，如果llm用于生成文本，而不仅仅是一个正确的文本呢？

3
00:00:18,040 --> 00:00:21,280
让我们看一下如何评估这种类型的llm输出的方法。

4
00:00:21,280 --> 00:00:26,400
这是我的常用辅助函数，给定客户消息，告诉我关于Smart X Pro手机和全栈相机等等。

5
00:00:26,400 --> 00:00:31,880
这里有一种方法来获取助手答案。

6
00:00:31,880 --> 00:00:37,040
这基本上是Yizer在早期视频中经历的过程。

7
00:00:37,040 --> 00:00:40,240
所以这是助手的答案。

8
00:00:40,240 --> 00:00:44,600
当然，我们有一个完整的智能手机，Smart X Pro手机等等。

9
00:00:44,600 --> 00:00:48,480
那么，如何评估这是否是一个好答案呢？

10
00:00:48,480 --> 00:00:50,040
似乎有很多可能的好答案。

11
00:00:50,040 --> 00:00:58,880
评估这一点的一种方法是编写一个评分表，即一组评估不同维度的指南，然后使用它来决定您是否满意这个答案。

12
00:00:58,880 --> 00:01:04,520
让我向您展示如何做到这一点。

13
00:01:04,520 --> 00:01:07,880
看起来有很多可能的好答案。

14
00:01:07,880 --> 00:01:16,560
评估这一点的一种方法是编写一个评分表，即一组评估不同维度的指南，然后使用它来决定您是否满意这个答案。

15
00:01:16,560 --> 00:01:21,560
让我向您展示如何做到这一点。

16
00:01:21,560 --> 00:01:22,560
让我向您展示如何做到这一点。

17
00:01:22,560 --> 00:01:26,040
让我向您展示如何做到这一点。

18
00:01:26,040 --> 00:01:29,680
所以让我创建一个小数据结构来存储客户消息以及产品信息。

19
00:01:29,680 --> 00:01:31,480
这里我将指定一个用于使用所谓的评分表评估助手答案的提示。

20
00:01:31,480 --> 00:01:38,200
这里我将指定一个用于使用所谓的评分表评估助手答案的提示。 

21
00:01:38,200 --> 00:01:39,200
让我向您展示如何做到这一点。
 
22
00:01:39,200 --> 00:01:41,200
我马上会解释那是什么意思。

23
00:01:41,200 --> 00:01:45,040
但是这个提示说，在系统消息中，你是一个助手，通过查看客服代理使用的上下文来评估客服代理回答用户问题的好坏。

24
00:01:45,040 --> 00:01:49,520
所以这个回答是我们在笔记本上方进一步得到的助手回答。

25
00:01:49,520 --> 00:01:52,200
我们将在这个提示中指定数据，即客户留言是什么，上下文是什么，即提供了什么产品和类别信息，然后是LLM的输出。

26
00:01:52,200 --> 00:01:55,880
然后这是一个评分标准。

27
00:01:55,880 --> 00:01:58,480
所以我们希望LLM将事实内容和提交的答案与内容进行比较，忽略风格、语法、标点符号上的差异。

28
00:01:58,480 --> 00:02:03,520
然后我们希望它检查一些事情，比如助手回答是否仅基于提供的上下文？

29
00:02:03,520 --> 00:02:08,760
答案是否包含上下文中未提供的信息？

30
00:02:08,760 --> 00:02:11,240
回答和上下文之间是否存在任何分歧？

31
00:02:11,240 --> 00:02:12,600
然后这是一个评分标准。

32
00:02:12,600 --> 00:02:16,360
所以我们希望LLM将事实内容和提交的答案与内容进行比较，忽略风格、语法、标点符号上的差异。

33
00:02:16,360 --> 00:02:19,200
然后我们希望它检查一些事情，比如助手回答是否仅基于提供的上下文？

34
00:02:19,200 --> 00:02:23,160
答案是否包含上下文中未提供的信息？

35
00:02:23,160 --> 00:02:25,200
回答和上下文之间是否存在任何分歧？

36
00:02:25,200 --> 00:02:28,800
所以这被称为评分标准，它指定了我们认为答案应该得到什么样的正确性才能被认为是一个好的答案。

37
00:02:28,800 --> 00:02:31,980
最后，我们希望它打印出是或否等等。

38
00:02:31,980 --> 00:02:39,240
现在，如果我们运行这个评估，这就是你得到的结果。

39
00:02:39,240 --> 00:02:42,840
它说助手回答是基于提供的上下文的。
 
43
00:03:03,280 --> 00:03:06,600
在这种情况下，它似乎没有创造新的信息。

44
00:03:06,600 --> 00:03:07,600
没有分歧。

45
00:03:07,600 --> 00:03:11,760
用户提出了两个问题，回答了问题一和问题二，因此回答了两个问题。

46
00:03:11,760 --> 00:03:14,120
所以我们会看这个输出，可能会得出这是一个相当好的回答。

47
00:03:14,120 --> 00:03:20,200
还有一点，这里我使用的是Chai GPT 3.5 Turbo模型进行评估。

48
00:03:20,200 --> 00:03:28,800
为了进行更全面的评估，可能值得考虑使用GPT-4，因为即使您在生产中部署了3.5 Turbo并生成了大量文本，如果您的评估是一个更为零散的练习，那么支付略微更昂贵的GPT-4 API调用来获得更严格的输出评估可能是明智的。

49
00:03:28,800 --> 00:03:34,680
我希望您能从中学到一个设计模式，即当您可以指定一个评估LLM输出的标准列表时，您实际上可以使用另一个API调用来评估您的第一个LLM输出。

50
00:03:34,680 --> 00:03:41,680
还有一种设计模式可能对某些应用程序有用，即如果您可以指定理想的响应。

51
00:03:41,680 --> 00:03:49,440
因此，在这里，我将指定一个测试示例，其中客户消息是“告诉我关于SmartX配置文件等的信息”。

52
00:03:49,440 --> 00:03:55,320
这是如果您有一个专业的人类客户服务代表编写一个非常好的答案。

53
00:03:55,320 --> 00:03:58,880
我希望您能从中学到一个设计模式，即当您可以指定一个评估LLM输出的标准列表时，您实际上可以使用另一个API调用来评估您的第一个LLM输出。

54
00:03:58,880 --> 00:04:06,680
还有一种设计模式可能对某些应用程序有用，即如果您可以指定理想的响应。

55
00:04:06,680 --> 00:04:12,440
因此，在这里，我将指定一个测试示例，其中客户消息是“告诉我关于SmartX配置文件等的信息”。

56
00:04:12,440 --> 00:04:19,200
这是如果您有一个专业的人类客户服务代表编写一个非常好的答案。

57
00:04:19,200 --> 00:04:23,000
如果您可以指定一个理想的响应，那么还有另一种设计模式可能对某些应用程序有用。

58
00:04:23,000 --> 00:04:28,080
因此，在这里，我将指定一个测试示例，其中客户消息是“告诉我关于SmartX配置文件等的信息”。

59
00:04:28,080 --> 00:04:31,120
这是一个理想的答案。

60
00:04:31,120 --> 00:04:32,840
如果您有一个专业的人类客户服务代表编写一个非常好的答案。

61
00:04:32,840 --> 00:04:38,240
因此，在这里，我将指定一个测试示例，其中客户消息是“告诉我关于SmartX配置文件等的信息”。

62
00:04:38,240 --> 00:04:39,240
这是一个理想的答案。
 
63
00:04:39,240 --> 00:04:41,600
专家说，这将是一个很好的答案。

64
00:04:41,600 --> 00:04:44,200
当然，还有SmartX配置文件等等。

65
00:04:44,200 --> 00:04:47,960
并继续提供许多有用的信息。

66
00:04:47,960 --> 00:04:54,640
现在，不合理地期望任何LLM生成这个精确的答案，逐字逐句。

67
00:04:54,640 --> 00:04:59,840
在经典的自然语言处理技术中，有一些传统的度量标准

68
00:04:59,840 --> 00:05:06,320
用于衡量LLM输出是否类似于这个专家人类编写的输出。

69
00:05:06,320 --> 00:05:10,960
例如，有一种称为BLU分数的东西，B-L-E-U，您可以在网上搜索

70
00:05:10,960 --> 00:05:12,360
了解更多信息。

71
00:05:12,360 --> 00:05:17,760
它们可以衡量一段文本与另一段文本的相似程度。

72
00:05:17,760 --> 00:05:25,600
事实证明，有一种更好的方法，就是您可以使用提示，我将在此指定提示，要求LLM比较自动生成的客户

73
00:05:25,600 --> 00:05:31,480
服务代理输出与上面由人类编写的理想专家响应的匹配程度。

74
00:05:31,480 --> 00:05:37,640
这是我们可以使用的提示，我们将使用LLM并告诉它成为一个助手

75
00:05:37,640 --> 00:05:41,320
通过比较自动生成的响应与理想的专家人类编写的响应来评估客户服务代理回答用户问题的好坏。

76
00:05:41,320 --> 00:05:47,280
这是我们可以使用的提示，我们将使用LLM并告诉它成为一个助手

77
00:05:47,280 --> 00:05:51,600
通过比较自动生成的响应与理想的专家人类编写的响应来评估客户服务代理回答用户问题的好坏。

78
00:05:51,600 --> 00:05:58,360
响应是自动生成的，理想的专家人类编写的响应是什么，然后我们的LLM实际输出了什么。

79
00:05:58,360 --> 00:06:03,520
因此，我们将提供数据，即客户请求是什么，理想的专家编写的响应是什么，然后我们的LLM实际输出了什么。
 
81
00:06:09,280 --> 00:06:15,480
这个评分标准来自于OpenAI开源评估框架，这是一个非常棒的框架，其中包含了许多评估方法，既有OpenAI开发人员的贡献，也有更广泛的开源社区的贡献。

82
00:06:15,480 --> 00:06:22,320
实际上，如果你愿意，你也可以为该框架贡献一个评估，以帮助他人评估他们的大型语言模型输出。

83
00:06:22,320 --> 00:06:23,320
因此，在这个评分标准中，我们告诉LLM比较提交的答案的事实内容和专家答案，忽略风格、语法、标点符号的差异，并随意暂停视频并详细阅读此内容。

84
00:06:23,320 --> 00:06:29,640
但关键是我们要求它进行比较，并输出从A到E的分数，具体取决于提交的答案是否是专家答案的子集，并且是否完全一致，或者提交的答案是否是专家答案的超集，但是它与专家答案完全一致。

85
00:06:29,640 --> 00:06:32,840
这可能意味着它虚构或编造了一些额外的事实。

86
00:06:32,840 --> 00:06:38,400
提交的答案包含了专家答案的所有细节，无论是否存在分歧或答案不同，但这些差异从实际角度来看并不重要。

87
00:06:38,400 --> 00:06:44,000
LLM将选择其中最合适的描述。

88
00:06:44,000 --> 00:06:46,880
这可能意味着它虚构或编造了一些额外的事实。

89
00:06:46,880 --> 00:06:54,320
提交的答案包含了专家答案的所有细节，无论是否存在分歧或答案不同，但这些差异从实际角度来看并不重要。

90
00:06:54,320 --> 00:06:59,480
LLM将选择其中最合适的描述。

91
00:06:59,480 --> 00:07:04,440
这可能意味着它虚构或编造了一些额外的事实。

92
00:07:04,440 --> 00:07:05,440
提交的答案包含了专家答案的所有细节，无论是否存在分歧或答案不同，但这些差异从实际角度来看并不重要。

93
00:07:05,440 --> 00:07:08,640
LLM将选择其中最合适的描述。

94
00:07:08,640 --> 00:07:16,600
这是提交的答案包含了专家答案的所有细节，无论是否存在分歧或答案不同，但这些差异从实际角度来看并不重要。

95
00:07:16,600 --> 00:07:22,880
LLM将选择其中最合适的描述。

96
00:07:22,880 --> 00:07:27,100
这可能意味着它虚构或编造了一些额外的事实。

97
00:07:27,100 --> 00:07:29,520
这是我们刚刚得到的助手答案。

98
00:07:29,520 --> 00:07:33,720
我认为这是一个相当好的答案，但现在让我们看看当它将助手答案与测试集ID进行比较时会发生什么。

99
00:07:33,720 --> 00:07:36,000

 
100
00:07:36,000 --> 00:07:41,960
哦，看起来它得到了A。因此，它认为提交的答案是专家答案的子集，并且与之完全一致。

101
00:07:41,960 --> 00:07:44,400
这听起来对我来说是正确的。

102
00:07:44,400 --> 00:07:45,400
这听起来对我来说是正确的。

103
00:07:45,400 --> 00:07:50,280
这个助手答案比上面的长专家答案要短得多，但它确实希望是一致的。

104
00:07:50,280 --> 00:07:51,280
希望它是一致的。

105
00:07:51,280 --> 00:07:59,000
再次强调，我在这个例子中使用的是GPT 3.5 turbo，但为了获得更严格的评估，

106
00:07:59,000 --> 00:08:04,400
在您自己的应用程序中使用GPT 4可能是有意义的。

107
00:08:04,400 --> 00:08:06,160
现在让我们尝试完全不同的东西。

108
00:08:06,160 --> 00:08:10,400
我将有一个非常不同的助手答案。

109
00:08:10,400 --> 00:08:15,560
人生就像一盒巧克力，引用自一部名为《阿甘正传》的电影。

110
00:08:15,560 --> 00:08:23,640
如果我们要评估它，它会输出D，并得出结论，提交的答案“人生就像一盒巧克力”与专家答案存在分歧。

111
00:08:23,640 --> 00:08:28,320
因此，它正确地评估这是一个相当糟糕的答案。

112
00:08:28,320 --> 00:08:32,160
所以就是这样。

113
00:08:32,160 --> 00:08:33,360
希望你从这个视频中学到两个设计模式。

114
00:08:33,360 --> 00:08:38,120
第一个是即使没有专家提供的理想答案，如果你能写一个评分标准，你可以使用一个LLM来评估另一个LLM的输出。

115
00:08:38,120 --> 00:08:44,840
第二，如果您可以提供一个专家提供的理想答案，那么可以帮助您的LLM更好地比较特定助手输出是否类似于专家提供的理想答案。

116
00:08:44,840 --> 00:08:48,720
我希望这可以帮助您评估LLM系统的输出，以便在开发期间

117
00:08:48,720 --> 00:08:55,600
和第二，如果您可以提供一个专家提供的理想答案，那么可以帮助您的LLM更好地比较特定助手输出是否类似于专家提供的理想答案。

118
00:08:55,600 --> 00:09:03,320
我希望这可以帮助您评估LLM系统的输出，以便在开发期间

119
00:09:03,320 --> 00:09:12,440
我希望这可以帮助您评估LLM系统的输出，以便在开发期间。
 
120
00:09:12,440 --> 00:09:17,800
当系统运行并且您正在收到响应时，您可以继续监视其性能，并拥有这些工具来持续评估和改进系统的性能。

121
00:09:17,800 --> 00:09:24,520
持续监测系统的性能，并使用这些工具不断评估和改进系统的性能。

122
00:09:24,520 --> 00:09:34,520
持续改进系统的性能。